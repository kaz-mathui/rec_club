1. Design a cache for a single system.

LRU Cache class with Ordered Dicts in Python.

2. Expand to many machines.

Option 1: Each machine has its own cache.

- This has the advantage of being relatively quick, since no
  machine-to-machine calls are used. The cache, unfortunately, is somewhat less
  effective as an optimization tool as many repeat queries would be treated
  as fresh queries.

Option 2: Each machine has a copy of the cache.

- This design means that common queries would nearly always be in the cache, as
  the cache is the same everywhere. The major drawback is that updating the
  cache means firing off data to N different machines, where N is the size of
  the response cluster. Additionally, because each item effectively takes up N
  times as much space, our cache would hold much less data.

Option 3: Each machine stores a segment of the cache.

- One option is to assign queries based on the formula hash(query) % N. Then,
  machine 1 only needs to apply this formula to know that machine j should store
  the results for this query.

3. Updating results when contents change.

- A good way to handle this is to implement an "automatic time-out" on the
  cache. That is, we'd impose a time out where no query, regardless of how
  popular it is, can sit in the cache for more than x minutes. This will
  ensure all data is periodically refreshed.

4. Further Enhancements.

- Support popular queries by instead of forwarding the request to machine j
  every time, machine i could forward it once and store the results in its own
  cache.
- We could implement timeouts based on topic or based on URLs (news sites
  update much more frequently).
