url to file database --> server with files
|
-----------------------> server with files
|
-----------------------> server with files

Here, we have a simple database that looks up the location (server and path)
of each file. When we have a request for a URL, we look up the location of the
URL within the datastore and then access the file.
Additionally, we will need a database that tracks analytics. We can do this
with a simple datastore that adds each visit (including timestamp, IP address
and location) as a row in a database. When we need to access the stats of each
visit, we pull the relevant data in from this database.

Key Issues:

We probably want to use a cache to store the most recently accessed documents.
This will ensure that items accessed very frequently (or very recently) will be
quickly accessible. Since documents cannot be edited, we will not need to worry
about invalidating this cache.

We should also potentially consider sharding the database.

We could skip the database entirely and just let a hash of the URL indicate
which server contains the document. One potential issue is that if we need to
add servers, it could be difficult to redistribute the documents.

Generating URLs
One simple path is to generate a random GUID. This is a 128-bit value that,
while not strictly gauranteed to be unique, has low enough odds of a collision
that we can treat it as unique. The drawback of this plan is that such a URL is
not very "pretty" to the user.

Or we could generate a 10-character sequence of letters and numbers, which
gives us 36\*\*10 possible strings.

Analytics

We have two options:

- Store the raw data from each visit
- Store just the data we know we'll use

It probably makes more sense to store the raw data. We never know what
features we'll add to the analytics down the road. The raw data allows
us flexibility.
