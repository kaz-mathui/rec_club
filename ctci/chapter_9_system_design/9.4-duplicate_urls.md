10 billion URLS x ~100 characters at 4 bytes = 4 terabytes

If we can hold all of this data in memory, we can use hash a table where each
URL maps to true if it has already been found.

Solution 1: Disk Storage
If we stored all data on one machine, we could do two passes of the document.
The first pass would split the list of URLs into 4000 chunks of 1 GB each. An
easy of doing that might be to store each URL in a file named {x}.txt where
x = hash(u) % 4000. That is, we divide up the URLS based on their hash value.
This way, all URLs with the same hash value would be in the same file.

In the second pass, we would essentually implement the simple solution:
load each file into memory, create a hash table of the URLs, and look for
duplicates.

Solution 2: Multiple Machines
The other solution is to perform the same procedure but to use multiple
machines.

The main pro is that we can parallelize the operation, such that 4000 chunks
are processed simultaneously. For large amounts of data, this might result in
a fast solution.

The disadvantage is that we are now relying on 4000 different machines to
operate perfectly. This may not be realistic and we'll need to start
considering how to handle failure.
