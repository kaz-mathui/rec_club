One way to tackle this is to have some sort of estimation for degree of
similarity. If, based on the content and the URL, a page is deemed to be
sufficiently similar to other pages, we deprioritize crawling its children.

We would have a database which stores a list of items we need to crawl. On
each iteration, we select the highest priority page to crawl.

- Open up the page and create a signature based on specific subsections of
  the content and the page's URL.
- Query the database to see whether anything with this signature has been
  crawled recently.
- If something with this signature has been recently crawled, insert this
  page back into the database at a low priority.
- If not, crawl the page and insert its link into the database.

Under this implementation, we never "complete" crawling the web, but we will
avoid getting stuck in a loop of pages. If we want to allow for the possibility
of "finishing", then we can set a minimum priority that a page must have have
to be crawled.
